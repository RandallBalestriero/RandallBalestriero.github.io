<img src="docs/assets/image_preprint_inversion.png" class="card-img-top" alt="faces generation">
<div class="card-body">
    <h6 class="card-title">New preprint on visualizing the feature space of deep networks</h6>
    <p class="card-text text-center">
        <a href="https://arxiv.org/pdf/2112.09164.pdf" class="p-1">Link</a>
        <a href="#" data-toggle="modal" data-target="#modal_inversion" class="p-1">
            Abstract
        </a>
    <div class="modal fade" id="modal_inversion" tabindex="-1" role="dialog" aria-labelledby="exampleModalLabel"
        aria-hidden="true">
        <div class="modal-dialog" role="document">
            <div class="modal-content">
                <h5 class="modal-title">
                    High Fidelity Visualization of What Your Self-Supervised Representation Knows About
                </h5>
                <div class="modal-body">
                    Discovering what is learned by neural networks remains a challenge. In self-supervised learning,
                    classification is the most common task used to evaluate how good a representation is. However,
                    relying only on such downstream task can limit our understanding of how much information is retained
                    in the representation of a given input. In this work, we showcase the use of a conditional diffusion
                    based generative model (RCDM) to visualize representations learned with self-supervised models. We
                    further demonstrate how this model's generation quality is on par with state-of-the-art generative
                    models while being faithful to the representation used as conditioning. By using this new tool to
                    analyze self-supervised models, we can show visually that i) SSL (backbone) representation are not
                    really invariant to many data augmentation they were trained on. ii) SSL projector embedding appear
                    too invariant for tasks like classifications. iii) SSL representations are more robust to small
                    adversarial perturbation of their inputs iv) there is an inherent structure learned with SSL model
                    that can be used for image manipulation.
                </div>
                <div class="modal-footer text-center">
                    <button type="button" class="btn btn-secondary" data-dismiss="modal">Close</button>
                </div>
            </div>
        </div>
    </div>
    </p>
</div>
<div class="card-footer">
    <small class="text-muted">Last updated: 12/20/2021</small>
</div>